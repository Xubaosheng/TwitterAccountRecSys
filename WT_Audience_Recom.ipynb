{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display # for markdown text\n",
    "import json # for json methods\n",
    "import pprint # to print human readable dictionary\n",
    "import pandas as pd # for visualizations\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # for TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def printmd(string):\n",
    "    display(Markdown(string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# FETCHING THE DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## All the topics in our database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'16': 'openhardware',\n",
      " '18': 'Data Science',\n",
      " '19': 'Big Data',\n",
      " '20': 'Artificial Intelligence',\n",
      " '21': 'Business Intelligence',\n",
      " '31': 'arduino',\n",
      " '32': 'raspberry pi',\n",
      " '33': '3d printer',\n",
      " '36': 'Deep Learning',\n",
      " '37': 'IoT',\n",
      " '38': '3d printing',\n",
      " '39': 'open hardware',\n",
      " '56': 'Wearable',\n",
      " '57': 'Sustainable finance',\n",
      " '59': 'Sustainable Finance',\n",
      " '60': 'Climate Finance',\n",
      " '61': 'Green Bonds',\n",
      " '62': 'Green Economy'}\n"
     ]
    }
   ],
   "source": [
    "topics = json.load(open('topics.txt'))\n",
    "pprint.pprint(topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## The parameters in our scenario\n",
    "We are observing an audience, which is defined by two constraints: a topic and a location. Our example is the audience in Italy interested in the topic: Arduino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "TOPIC_ID=31 # topic = arduino\n",
    "LOCATION = 'italy'\n",
    "SIGNAL_STRENGTH = 0 # this value indicates the min number of influencers the retrieved audience members follow within the topic\n",
    "LIMIT = 30 # number of audience members to consider\n",
    "TESTING_SET_SIZE=10\n",
    "HOW_MANY_TWEETS = 50 # amount of most recent tweets (including retweets) to be retrieved to consider in our recommendation engine\n",
    "INCLUDE_RETWEETS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rated_audience_dict = json.load(open('rated_audience.txt'))\n",
    "unrated_audience_dict = json.load(open('unrated_audience.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## An example Twitter profile with all the data fields at this point."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Topic: arduino"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'description': 'Chief Innovation Officer at NTT DATA Italia , proud father of '\n",
      "                'two beautiful girls, a professor, a technology evangelist, an '\n",
      "                'holistic thinker and a gentleman.',\n",
      " 'ground_truth_rating': 1.0,\n",
      " 'hashtags': 'Milano CheTempoFa Milano CheTempoFa foi13 Cefriel fvw2013 '\n",
      "             'fvw2013 fvw2013 storytelling Vajont Milano CheTempoFa '\n",
      "             'StartupWeekend GrandC4Picasso makerfairerome MakerFaireRome '\n",
      "             'GrandC4Picasso GrandC4Picasso',\n",
      " 'influencers': '266400754 84094835 767285',\n",
      " 'location': 'Milan, Italy',\n",
      " 'screen_name': 'funkysurfer',\n",
      " 'tweets': 'at Cascina Matiot Disorders of Con Edi Touch is easier than yours '\n",
      "           'via Startup weekend 3 days at Milan Decoded Milan capital of the '\n",
      "           'street October 2013 Rain Massima Minima The new science is an open '\n",
      "           'narration thanks to This project was born via Milan handmade with '\n",
      "           'Craft Camp of a Pisa ItCup A Roncade in Veneto via October 2013 PM '\n",
      "           'Showers Maximima Minima at Frijenno Magnanno Want to come to check '\n",
      "           'out the offer that has been reserved for Makers vs. Todd Blatt is '\n",
      "           'a mechanical engineer from Baltimore who is certainly interested '\n",
      "           'in via General Meeting Classroom Osvaldo De Big the quality is '\n",
      "           'more important than the Linkedin has one of the best team of via '\n",
      "           'FabLab at school for the future will be talked about the future '\n",
      "           'via startups Future Pills on Startups and via A Facebook we '\n",
      "           'proceed to Stanford for Living with the net wants away Today a '\n",
      "           'lady approached me and she will Be the resemblance I drink only in '\n",
      "           'the days beginning with The translation for CHEEKY is Sfaccimme '\n",
      "           'dixit at on stage here at talking about Pirelli social life Elf '\n",
      "           'Puccini Theater 7 Smart City a workplace and From Boston to via '\n",
      "           'access is an Or it is the peer access is a way the Incubator '\n",
      "           'Network via Festival via the first steps All our way Because '\n",
      "           'artisans and makers find it hard to talk about what to do After '\n",
      "           'the success away tells us and Qesse Academy - when it educates and '\n",
      "           'emancipates Today 50 years of tragedy of the visit the memorial 1 '\n",
      "           'card x each of the 1910 E counterclaims The city of tomorrow in '\n",
      "           'the race at Solar Decathlon Density convenience via Sapienza focus '\n",
      "           'on the production of novelties and is a way October 2013 Rain '\n",
      "           'Maxima Minima Thinking about my friend Marco - and what the future '\n",
      "           'will be A Google with Hal Then at the research site never forget '\n",
      "           'via Italian tour diary in Silicon The lucky ones have slept three '\n",
      "           'way For our Coach is ready to help you to realize your Korean info '\n",
      "           'to the good use of Yuhyun Park has 38 a way Social Finance and '\n",
      "           'today Uman Foundation to the To get out of the crisis via At the '\n",
      "           'fair of two Arduino the maker at the Liceo scientifico Filippo '\n",
      "           'Lussana Brand and video 7 tips for a distribution in Italy if you '\n",
      "           'want the winner of TechCrunch When a few days in the grand finale '\n",
      "           'of the media and this morning the point of the We turned to see '\n",
      "           'you at the next Maker Faire at Milano Malpensa Airport of Nuove - '\n",
      "           'Somma 24 others Thank you for telling us about your 35 thousand '\n",
      "           'people at Grazie a Still five hours before you had fun and changed '\n",
      "           'the two of them to thank you parked by CC one can use the The odd '\n",
      "           'couple I am no longer able to drive with the manual gearbox and '\n",
      "           'have survived the driving of'}\n"
     ]
    }
   ],
   "source": [
    "printmd(\"## An example Twitter profile with all the data fields at this point.\")\n",
    "printmd(\"### Topic: \" + topics[str(TOPIC_ID)])\n",
    "pprint.pprint(next (iter (rated_audience_dict.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Computes the TF-IDF values for the given corpus.\n",
    "tf = TfidfVectorizer(analyzer='word', ngram_range=(1,1), min_df = 0, stop_words = 'english');\n",
    "def get_tfidf(corpus):\n",
    "    return tf.fit_transform(corpus.copy()).todense();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Separate the data into different arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "screen_names = [aud['screen_name'] for aud in rated_audience_dict.values()]\n",
    "influencers_corpus = [aud['influencers'] for aud in rated_audience_dict.values()]\n",
    "tweets_corpus = [aud['tweets'] for aud in rated_audience_dict.values()]\n",
    "hashtags_corpus = [aud['hashtags'] for aud in rated_audience_dict.values()]\n",
    "description_corpus = [aud['description'] for aud in rated_audience_dict.values()]\n",
    "ground_truth_ratings = np.array([2*aud['ground_truth_rating'] for aud in rated_audience_dict.values()])\n",
    "#print(tweets_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Matrix dimensions"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Influencer matrix: (21, 30)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Tweets matrix: (3539, 30)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Description matrix: (209, 30)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Hashtags matrix: (778, 30)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# INFLUENCER MATRICES\n",
    "tfidf_influencer_matrix = np.array(get_tfidf(influencers_corpus)).T;\n",
    "binary_influencer_matrix = tfidf_influencer_matrix.copy()\n",
    "binary_influencer_matrix[binary_influencer_matrix>0]=1\n",
    "# TWEET MATRICES\n",
    "tfidf_tweets_matrix=np.array(get_tfidf(tweets_corpus)).T;\n",
    "binary_tweets_matrix = tfidf_tweets_matrix.copy()\n",
    "binary_tweets_matrix[binary_tweets_matrix>0]=1\n",
    "# DESCRIPTION MATRICES\n",
    "tfidf_description_matrix = np.array(get_tfidf(description_corpus)).T;\n",
    "binary_description_matrix = tfidf_description_matrix.copy()\n",
    "binary_description_matrix[binary_description_matrix>0]=1\n",
    "# HASHTAGS MATRICES\n",
    "tfidf_hashtags_matrix = np.array(get_tfidf(hashtags_corpus)).T;\n",
    "binary_hashtags_matrix = tfidf_hashtags_matrix.copy()\n",
    "binary_hashtags_matrix[binary_hashtags_matrix>0]=1\n",
    "printmd(\"#### Matrix dimensions\")\n",
    "printmd(\"Influencer matrix: \" + str(tfidf_influencer_matrix.shape))\n",
    "printmd(\"Tweets matrix: \" +str(tfidf_tweets_matrix.shape))\n",
    "printmd(\"Description matrix: \" +str(tfidf_description_matrix.shape))\n",
    "printmd(\"Hashtags matrix: \" +str(tfidf_hashtags_matrix.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# METHODS FOR RECOMMENDATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Method A\n",
    "## Logistic Regression using the similarities coming from user profiling (fixed parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from tfidf_vectorizer import TwitterAccountSimilarityFinder\n",
    "import mord as md\n",
    "from sklearn import datasets, linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "def get_user_profile(tfidf_matrix, ratings):\n",
    "    return np.multiply(tfidf_matrix.T,ratings[:, np.newaxis]).sum(axis=0).reshape(1,tfidf_matrix.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def cos_sim(tfidf_matrix, user_profile):\n",
    "    norm = np.linalg.norm(user_profile);\n",
    "    return 1.0* tfidf_matrix.T.dot(user_profile.T)/ norm\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SUBSAMPLING_COUNT = 100 # we will subsample this many times and take the average error for evaluation\n",
    "rated_audience_dict_ids = list(rated_audience_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Training Mean Squared Error: 2.1665\n",
      "Avg Testing Mean Squared Error: 15.361\n"
     ]
    }
   ],
   "source": [
    "mean_square_errors = []\n",
    "prints_enabled = False\n",
    "for iteration in range(SUBSAMPLING_COUNT):    \n",
    "    TESTING_SET_IDS = np.random.choice(rated_audience_dict_ids, size=TESTING_SET_SIZE, replace=False)\n",
    "    if (prints_enabled): print(TESTING_SET_IDS)\n",
    "    ratings = np.array([0 if id in TESTING_SET_IDS else int(2*aud['ground_truth_rating']) for id, aud in rated_audience_dict.items()])\n",
    "    if (prints_enabled): display(ratings)\n",
    "        \n",
    "    # FIND USER PROFILES\n",
    "    influencer_user_profile = get_user_profile(tfidf_influencer_matrix, ratings)\n",
    "    tweets_user_profile = get_user_profile(tfidf_tweets_matrix, ratings)\n",
    "    description_user_profile = get_user_profile(tfidf_description_matrix, ratings)\n",
    "    hashtags_user_profile = get_user_profile(tfidf_hashtags_matrix, ratings)\n",
    "\n",
    "    # FIND THE COSINE SIMILARITIES\n",
    "    # THESE WILL THEN BE USED AS FEATURES IN REGRESSION\n",
    "    influencerSimilarities = cos_sim(tfidf_influencer_matrix, influencer_user_profile)\n",
    "    tweetSimilarities = cos_sim(tfidf_tweets_matrix, tweets_user_profile)\n",
    "    descriptionSimilarities = cos_sim(tfidf_description_matrix, description_user_profile)\n",
    "    hashtagSimilarities = cos_sim(tfidf_hashtags_matrix, hashtags_user_profile)\n",
    "    \n",
    "    profile_count = len(rated_audience_dict)\n",
    "    avgTweetSim = np.mean([e for e in tweetSimilarities if e!=0]) # if we cannot fetch tweets of a profile, we assign his tweets an average similarity score\n",
    "    avgDescriptionSim = np.mean([e for e in descriptionSimilarities if e!=0]) # if the description of a profile is empty, we assign his descripion an average similarity score\n",
    "    avgHashtagSim = np.mean([e for e in hashtagSimilarities if e!=0]) # if we cannot fetch tweets of a profile, we assign his hashtags an average similarity score\n",
    "    \n",
    "    profiles =[]\n",
    "    training_indices=[]\n",
    "    for i in range(profile_count):\n",
    "        if tweetSimilarities[i]==0: tweetSimilarities[i]= avgTweetSim\n",
    "        if descriptionSimilarities[i]==0: descriptionSimilarities[i]= avgDescriptionSim\n",
    "        if hashtagSimilarities[i]==0: hashtagSimilarities[i] = avgHashtagSim\n",
    "        if ratings[i]!=0: training_indices.append(i)\n",
    "        profile = {\n",
    "        'index': i,\n",
    "        'screen_name':screen_names[i],\n",
    "        'set': \"testing\" if ratings[i]==0 else \"training\",\n",
    "        'ground_truth':ground_truth_ratings[i],\n",
    "        'infSim': influencerSimilarities[i].item(0),\n",
    "        'tweetSim': tweetSimilarities[i].item(0),\n",
    "        'descSim': descriptionSimilarities[i].item(0),\n",
    "        'hashtagSim': hashtagSimilarities[i].item(0),\n",
    "        'score': 0\n",
    "        }\n",
    "        count=0\n",
    "        for key in profile.keys():\n",
    "            if 'Sim' in key: \n",
    "                profile['score']+=profile[key]\n",
    "                count+=1\n",
    "        profile['score']/=count*1.0\n",
    "        profiles.append(profile)\n",
    "\n",
    "    profiles=pd.DataFrame(profiles)\n",
    "    if (prints_enabled):\n",
    "        display(profiles[['screen_name','set','ground_truth','score','infSim','tweetSim','descSim','hashtagSim']].sort_values(by='score',ascending=False).round(2))\n",
    "        printmd(\"### Naive approach\\n Score is the average of similarities.\")\n",
    "        profiles.plot.scatter('score','ground_truth') # score is the average of the similarities\n",
    "    \n",
    "    # Combine the similarities and use them as features to feed to a logistic regressor.\n",
    "    # uncomment to add the similarity into regression\n",
    "\n",
    "    featureVectors = influencerSimilarities\n",
    "    featureVectors = np.column_stack((featureVectors,tweetSimilarities))\n",
    "    #featureVectors = np.column_stack((featureVectors, descriptionSimilarities))\n",
    "    #featureVectors = np.column_stack((featureVectors, hashtagSimilarities))\n",
    "\n",
    "    X = np.array([featureVectors[i] for i in training_indices])\n",
    "    X_binary = X.copy()\n",
    "    X_binary[X_binary>0]=1\n",
    "    \n",
    "    Y = np.array([ratings[i] for i in training_indices])\n",
    "    Y_binary = [round(e/10) for e in Y]\n",
    "    \n",
    "    # Ordinal Regression\n",
    "    #classifier = linear_model.LinearRegression()\n",
    "    # Logistic Regression\n",
    "    #classifier = md.LogisticIT() #Default parameters: alpha=1.0, verbose=0, maxiter=10000\n",
    "    \n",
    "    classifier = linear_model.LogisticRegression(C=1e5)\n",
    "    #print(X)\n",
    "    #print(Y)\n",
    "    classifier.fit(X, Y)\n",
    "\n",
    "    predictions = classifier.predict(featureVectors)\n",
    "    profiles['predicted_rating']=predictions\n",
    "    profiles[\"squared_error\"]=(profiles[\"ground_truth\"]-profiles[\"predicted_rating\"])**2\n",
    "    if (prints_enabled):\n",
    "        display(profiles[['screen_name','set','ground_truth', 'predicted_rating','squared_error','score','infSim','tweetSim','descSim','hashtagSim']].sort_values(by='score',ascending=False).round(2))\n",
    "\n",
    "    evaluation = pd.DataFrame()\n",
    "    evaluation['mean_squared_error']=profiles.groupby(by='set')['squared_error'].mean()\n",
    "    mean_square_error = {\n",
    "        'training':evaluation.filter(like='training', axis=0)['mean_squared_error'].iloc[0],\n",
    "        'testing':evaluation.filter(like='testing', axis=0)['mean_squared_error'].iloc[0],\n",
    "        }\n",
    "    mean_square_errors.append(mean_square_error)\n",
    "    \n",
    "Avg_MSE_training = np.mean([e['training'] for e in mean_square_errors])\n",
    "Avg_MSE_testing = np.mean([e['testing'] for e in mean_square_errors])\n",
    "print(\"Avg Training Mean Squared Error: \" + str(Avg_MSE_training))\n",
    "print(\"Avg Testing Mean Squared Error: \" + str(Avg_MSE_testing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Method B\n",
    "## Logistic Regression using the similarities coming from user profiling (relaxed parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Method C \n",
    "## Non-negative Matrix Factorization (NMF) without the ratings row = Document Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Matrix Factorization via multiplicative update rule\n",
    "# Original Author: Ali Taylan Cemgil from Bogazici University\n",
    "def nmf_kl_multiplicative(D, M, W, H, EPOCH=5000):\n",
    "    MD = D.copy()\n",
    "    MD[M==0] = 0\n",
    "    for e in range(EPOCH):\n",
    "        Xhat = W.dot(H)\n",
    "        W=W*np.array(((MD/Xhat).dot(H.T)/np.dot(M, H.T)))\n",
    "        Xhat = W.dot(H)\n",
    "        H = H*np.array((W.T.dot(MD/Xhat)/np.dot(W.T, M)))\n",
    "        #print(np.sum(np.abs(MD - M*Xhat))/np.sum(M))\n",
    "    return W, H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Regularized Matrix Factorization \n",
    "def matrix_factorization(R, P, Q, K, steps=5000, alpha=0.0002, beta=0.02):\n",
    "    Q = Q.T\n",
    "    for step in range(steps):\n",
    "        for i in range(len(R)):\n",
    "            for j in range(len(R[i])):\n",
    "                if R[i][j] > 0:\n",
    "                    eij = R[i][j] - np.dot(P[i,:],Q[:,j])\n",
    "                    for k in range(K):\n",
    "                        if(i==0):\n",
    "                            P[i][k] = P[i][k] + alpha * (4 * eij * Q[k][j] - beta * P[i][k])\n",
    "                        else:         \n",
    "                            P[i][k] = P[i][k] + alpha * (2 * eij * Q[k][j] - beta * P[i][k])\n",
    "                        if(k==0):\n",
    "                            Q[k][j] = Q[k][j] + alpha * (4 * eij * P[i][k] - beta * Q[k][j])\n",
    "                        else:\n",
    "                            Q[k][j] = Q[k][j] + alpha * (2 * eij * P[i][k] - beta * Q[k][j])\n",
    "\n",
    "        eR = np.dot(P,Q)\n",
    "        e = 0\n",
    "        for i in range(len(R)):\n",
    "            for j in range(len(R[i])):\n",
    "                if R[i][j] > 0:\n",
    "                    e = e + pow(R[i][j] - np.dot(P[i,:],Q[:,j]), 2)\n",
    "                    for k in range(K):\n",
    "                        e = e + (beta/2) * (pow(P[i][k],2) + pow(Q[k][j],2))\n",
    "        if e < 0.001:\n",
    "            break\n",
    "    return P, Q.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf_data_matrix = tfidf_influencer_matrix # first add influencers\n",
    "#tfidf_data_matrix = np.append(data_matrix,tfidf_tweet_matrix,axis=0) # then tweets\n",
    "#tfidf_data_matrix = np.append(data_matrix,tfidf_description_matrix,axis=0) # then descriptions\n",
    "#tfidf_data_matrix = np.append(data_matrix,tfidf_hashtags_matrix,axis=0) # then hashtags\n",
    "#display(pd.DataFrame(tfidf_data_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1751981' '69301653' '7936922' '84250273' '16706425' '74806908' '54360528'\n",
      " '14184828' '15180646' '18049877']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 2,  0,  0,  4,  5,  0, 10,  0,  3,  0,  6,  8,  0,  3,  2,  2,  5,\n",
       "        2,  0,  5,  8,  2,  0,  4,  0,  9,  5,  0,  4,  3])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster_no</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>4.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>4.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>4.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>4.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cluster_no     rating\n",
       "0            0   7.000000\n",
       "1            1   5.000000\n",
       "2            2   1.000000\n",
       "3            3   0.000000\n",
       "4            4   2.500000\n",
       "5            5   8.000000\n",
       "6            6   8.000000\n",
       "7            7   5.000000\n",
       "8            8   4.500000\n",
       "9            9   0.000000\n",
       "10          10   6.000000\n",
       "11          11  10.000000\n",
       "12          12   4.500000\n",
       "13          13   4.333333\n",
       "14          14   3.000000\n",
       "15          15   4.666667\n",
       "16          16   2.000000\n",
       "17          17   3.000000\n",
       "18          18   5.000000\n",
       "19          19   0.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean_square_errors = []\n",
    "prints_enabled = True\n",
    "#Rank\n",
    "R = 20\n",
    "# Data\n",
    "Nr = tfidf_data_matrix.shape[0]\n",
    "Nc = tfidf_data_matrix.shape[1]\n",
    "\n",
    "for iteration in range(1):    \n",
    "    TESTING_SET_IDS = np.random.choice(rated_audience_dict_ids, size=TESTING_SET_SIZE, replace=False)\n",
    "    if (prints_enabled): print(TESTING_SET_IDS)\n",
    "    ratings = np.array([0 if id in TESTING_SET_IDS else int(2*aud['ground_truth_rating']) for id, aud in rated_audience_dict.items()])\n",
    "    if (prints_enabled): display(ratings)\n",
    "\n",
    "    # Initialize W and H with random numbers\n",
    "    W = np.random.rand(Nr, R)*100\n",
    "    H = np.random.rand(R, Nc)*100\n",
    "\n",
    "    Mask = np.ones_like(tfidf_data_matrix)\n",
    "    Mask[np.isnan(tfidf_data_matrix)] = 0\n",
    "\n",
    "    W,H = nmf_kl_multiplicative(tfidf_data_matrix, Mask, W, H, EPOCH=10)\n",
    "\n",
    "    # Cluster numbers holds which cluster each user is assigned to\n",
    "    cluster_numbers = np.argmax(H,axis=0)\n",
    "    # Ratings each cluster is assigned to (initialized to zeros)\n",
    "    cluster_ratings = np.zeros(R)\n",
    "\n",
    "    for i in range(len(cluster_numbers)):\n",
    "        cluster_no = cluster_numbers[i]\n",
    "        cluster_ratings[cluster_no]+=1.0*ground_truth_ratings[i] \n",
    "\n",
    "    for i in range(R):\n",
    "        cluster_size = float(len(cluster_numbers[cluster_numbers==i]))\n",
    "        if cluster_size!=0:\n",
    "            cluster_ratings[i]/=cluster_size\n",
    "   \n",
    "        \n",
    "    # Found the cluster ratings\n",
    "    cluster_ratings = [{'cluster_no':i, 'rating':cluster_ratings[i]} for i in range(R)]\n",
    "\n",
    "    cluster_ratings = pd.DataFrame(cluster_ratings)\n",
    "    display(cluster_ratings)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Method D \n",
    "## Non-negative Matrix Factorization (NMF) with the ratings row (more weight on errors caused by row 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Method E \n",
    "## Non-negative Matrix Factorization (NMF) with the ratings row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3env",
   "language": "python",
   "name": "python3env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
